import { useState } from "react";
import api from "../api/items";

// âœ… Vercel 413 í”¼í•˜ë ¤ê³  ë„‰ë„‰í•˜ê²Œ ë‚®ê²Œ ì¡ê¸° (0.8MB)
const MAX_BYTES = 800_000;

// ë°”ì´íŠ¸ ê¸°ì¤€ ì²­í¬
function chunkByBytes(list, wrapKey, maxBytes = MAX_BYTES) {
  const enc = new TextEncoder();
  const chunks = [];
  let cur = [];

  for (const obj of list) {
    const testPayload = { [wrapKey]: [...cur, obj] };
    const bytes = enc.encode(JSON.stringify(testPayload)).length;

    if (bytes > maxBytes) {
      if (cur.length === 0) {
        // ì´ê±´ â€œí•œ ê°œ ë°ì´í„°ê°€ í˜¼ìì„œë„ ë„ˆë¬´ í¼â€ (ëŒ€ë¶€ë¶„ base64 ì´ë¯¸ì§€)
        throw new Error(
          `Single ${wrapKey} item too large (>${maxBytes} bytes). ` +
          `imageUrl/base64 ê°™ì€ í° í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì œê±°í•´ì•¼ í•´.`
        );
      }
      chunks.push(cur);
      cur = [obj];
    } else {
      cur.push(obj);
    }
  }

  if (cur.length) chunks.push(cur);
  return chunks;
}

// âœ… 1ì°¨ ë§ˆì´ê·¸ë ˆì´ì…˜ì—ì„œ í° í•„ë“œ ì œê±° (í•„ìš”í•œ ìµœì†Œ í•„ë“œë§Œ ë‚¨ê¹€)
// (ë„ˆ ì„œë²„ itemsBatchHandlerê°€ ë°›ëŠ” í•„ë“œ ê¸°ì¤€ìœ¼ë¡œ êµ¬ì„±)
function sanitizeItem(it, userId) {
  const imageUrl = it.imageUrl ?? it.image ?? null;

  // base64ë©´ ì¼ë‹¨ ì œì™¸ (ë‚˜ì¤‘ì— ì´ë¯¸ì§€ë§Œ ë”°ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜)
  const safeImageUrl =
    typeof imageUrl === "string" && imageUrl.startsWith("data:image/")
      ? null
      : imageUrl;

  return {
    userId, // âœ… ê¼­ ë„£ê¸° (ì„œë²„ì—ì„œ it.userIdë¡œ ì €ì¥ ì¤‘)
    name: it.name ?? "",
    size: it.size ?? "",
    category: it.category ?? it.type ?? "FOOD",
    legacyId: String(it.legacyId ?? it.id ?? ""),
    imageUrl: safeImageUrl,
    createdAt: it.createdAt ?? it.created_at ?? null,
  };
}

function sanitizeRecord(r, userId) {
  return {
    userId, // âœ… recordsBatchHandlerê°€ records[0].userIdë¥¼ ìš”êµ¬í•¨
    // ë„ˆ ì„œë²„ ì½”ë“œê°€ shoeId ë˜ëŠ” itemLegacyIdë¥¼ legacyë¡œ ì“°ê³  ìˆìŒ
    shoeId: r.shoeId ?? r.itemId ?? r.itemLegacyId ?? r.legacyItemId,
    itemLegacyId: r.itemLegacyId ?? r.shoeId ?? r.itemId ?? r.legacyItemId,
    price: r.price,
    count: r.count,
    date: r.date ?? r.createdAt ?? r.created_at,
    category: r.category ?? null,
  };
}

export default function MigratePage() {
  const [file, setFile] = useState(null);
  const [log, setLog] = useState([]);
  const [loading, setLoading] = useState(false);

  function pushLog(msg) {
    setLog((prev) => [...prev, msg]);
  }

  async function handleMigrate() {
    if (!file) return alert("íŒŒì¼ ì„ íƒí•´ì¤˜");

    setLoading(true);
    setLog([]);

    try {
      // âœ… (ì¤‘ìš”) ë¡œê·¸ì¸ ëìœ¼ë‹ˆ, í˜„ì¬ ìœ ì € userId ë°›ì•„ì˜¤ê¸°
      // ë„ˆ í”„ë¡œì íŠ¸ì—ì„œ me ì—”ë“œí¬ì¸íŠ¸ê°€ ë­ì˜€ëŠ”ì§€ì— ë”°ë¼ í•˜ë‚˜ë§Œ ë§ì¶”ë©´ ë¨.
      // 1) /users/me  2) /me  3) /auth/me ì¤‘ í•˜ë‚˜
      let me;
      try {
        me = (await api.get("/users/me")).data;
      } catch {
        me = (await api.get("/me")).data;
      }
      const userId = me?.id ?? me?.userId;
      if (!userId) throw new Error("í˜„ì¬ ë¡œê·¸ì¸ ìœ ì €ì˜ userIdë¥¼ ëª» ê°€ì ¸ì™”ì–´. /me ì‘ë‹µì„ í™•ì¸í•´ì¤˜.");

      const text = await file.text();
      const json = JSON.parse(text);

      const stores = json.stores || {};

      // 1ï¸âƒ£ items ë¨¼ì €
      const rawItems = [...(stores.shoes || []), ...(stores.foods || [])];
      const items = rawItems.map((it) => sanitizeItem(it, userId)).filter((x) => x.name && x.size && x.legacyId);

      pushLog(`ğŸ“¦ items ${items.length}ê°œ ì—…ë¡œë“œ ì‹œì‘ (MAX_BYTES=${MAX_BYTES})`);

      const itemChunks = chunkByBytes(items, "items", MAX_BYTES);
      for (let i = 0; i < itemChunks.length; i++) {
        await api.post("/migrate/items-batch", { items: itemChunks[i] });
        pushLog(`âœ… items ${i + 1}/${itemChunks.length} ì™„ë£Œ (sent=${itemChunks[i].length})`);
      }

      // 2ï¸âƒ£ records ë‚˜ì¤‘
      const rawRecords = [...(stores.records || []), ...(stores.foodRecords || [])];
      const records = rawRecords.map((r) => sanitizeRecord(r, userId));

      pushLog(`ğŸ“¦ records ${records.length}ê°œ ì—…ë¡œë“œ ì‹œì‘ (MAX_BYTES=${MAX_BYTES})`);

      const recordChunks = chunkByBytes(records, "records", MAX_BYTES);
      for (let i = 0; i < recordChunks.length; i++) {
        await api.post("/migrate/records-batch", { records: recordChunks[i] });
        pushLog(`âœ… records ${i + 1}/${recordChunks.length} ì™„ë£Œ (sent=${recordChunks[i].length})`);
      }

      pushLog("ğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ");
    } catch (err) {
      console.error(err);
      alert(err?.response?.data?.message || err?.message || "ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨");
    } finally {
      setLoading(false);
    }
  }

  return (
    <div style={{ maxWidth: 520, margin: "40px auto" }}>
      <h2>ğŸ“¦ IndexedDB â†’ ì„œë²„ ë§ˆì´ê·¸ë ˆì´ì…˜</h2>

      <input type="file" accept=".json" onChange={(e) => setFile(e.target.files[0])} />

      <button onClick={handleMigrate} disabled={loading} style={{ marginTop: 16 }}>
        {loading ? "ì´ë™ ì¤‘..." : "ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘"}
      </button>

      <pre style={{ marginTop: 20, fontSize: 12 }}>{log.join("\n")}</pre>
    </div>
  );
}